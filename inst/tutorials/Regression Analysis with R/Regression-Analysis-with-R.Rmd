---
title: "Introduction to Regression Analysis with R"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)

set.seed(123)
par(mar = c(4,4,1,1))
nSamples <- 250
height<-rnorm(nSamples, 180, 20)
sex<-as.factor(sample(c("male", "female"), size = nSamples, replace = TRUE, prob = c(0.45,0.55)))
height<-height + rnorm(nSamples, 10,5)*(as.numeric(sex)-1)
age<-floor(runif(nSamples, 40,60))
weight<- height * 0.7 - 44 + rnorm(nSamples,0,12)

weight<-weight + rnorm(nSamples, 3, 2)*(as.numeric(sex)-1) + rnorm(nSamples, 0.005, 0.001)*(as.numeric(sex)-1) * height
weight <- weight + age * rnorm(nSamples, 0.04, 0.03)
bmi <- weight/(height/100)^2

smoker<-sample(c(0,1), size = nSamples, replace = TRUE, prob = c(0.8,0.2))
t2diabetes <- sample(c(0,1), size = nSamples, replace = TRUE, prob = c(0.8,0.2))
t2diabetes[sample(which(bmi > 25),10)]<-1
t2diabetes[sample(which(smoker == 1),5)]<-1

exercise_hours <- rpois(nSamples, 1) + rpois(nSamples, 2)*(1-t2diabetes) + rpois(nSamples, 1) * (as.numeric(sex)-1)
alcohol_units <- rpois(nSamples, 3) + rpois(nSamples, 5)*(1-t2diabetes) + rpois(nSamples, 3) * (as.numeric(sex)-1) + rpois(nSamples, 1)*rpois(nSamples, 6)*(1-t2diabetes) 
alcohol_units[which(bmi > 37)]<-alcohol_units[which(bmi > 37)] + rpois(sum(bmi > 37),5)
alcohol_units[which(weight > 140)]<-rpois(sum(weight > 140),50)

demoDat <-data.frame(age, height, weight, bmi, smoker, exercise_hours, alcohol_units, t2diabetes)

```

## Overview of Workshop

Welcome to Introductory Regression Analysis with R. Our aim is to provide you with a comprehensive introduction to the statistical tool regression. In this session you will learn what regression is, how to fit a range of regression models with R, how to interpret the output and the link between regression and other common statistical tools.

## Introduction to Regression

### What is Regression?

Regression analysis is a broad category of analyses where the objective is to statistically quantify relationships between variables.

It enables you to:

* understand which variables affect other variables and how
* make predictions from a new set of data

You might also know it as fitting a model to data. Where model is a mathematical specification of the relationship between the variables and we use our data to determine what numbers the model should include. 

It requires:

* dependent variable(s) - the outcome or variable you are trying to predict
* independent variable(s) - the predictors, features, covariates or explanatory variable(s)

You may also know it as fitting a line to data as in the example below, where the line is a graphical representation of a model. Note by line we are not limited to just a straight line.

```{r straight line example, echo = FALSE, fig.height= 4, fig.width=4}
set.seed(123)
par(mar = c(4,4,1,1))
nSamples <- 25
height<-rnorm(nSamples, 180, 20)
weight<- height * 0.62 - 44 + rnorm(nSamples,0,5)

plot(height, weight, pch = 16, col = , xlab = "Height (cm)", ylab = "Weight (kg)")
abline(a = -44, b = 0.62)
```


### What is a line?

To understand a bit more about regression parameters we are going to recap what a line, specifically a straight line, is in a mathematical sense. If we have two continuous variables such as height and weight, measured on the same set of individuals we can visualise the relationship with a scatterplot like the one above and try to generalise it by drawing a straight line through the points. From this line we can make predictions of what weight a person might have if we know their height. 

To do this we need to represent this relationship as an equation. The standard equation for a straight line between two variables Y and X:

$$Y = \theta_{0}  + \theta_{1} X$$

There are two regression parameters in this equation: 

1. Intercept ($\theta_{0}$ ) - This is the value of the outcome variable when the predictor is set to 0. 
2. Slope coefficient ($\theta_{1}$) - This is the change in the outcome variable for each unit of the predictor variable.

When we know the values of these parameters we can then input different values of X to make predictions for Y. What's more hanging the values of these parameters changes how the line appears. 

```{r straight line multiple, echo = FALSE, fig.height= 6, fig.width=6}
par(mar = c(4,4,1,1))
par(mfrow = c(2,2))

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + 5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 10, b = 5)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + -5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 10, b = -5)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + 10X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 5, b = 10)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 5 + 5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 5, b = 5)
abline(v = 0)
abline(h = 0)
```

Above we showcase a number of different lines. What you should observe is that  changing $theta_{1}$ changes the slope of the line. We can change the direction of the line depending if the parameter is negative or positive, we can change the steepness of the slope changing the magnitude of this parameter. This is the parameter that captures the relationship between our variables X and Y and enables us to model infinite linear relationships.  

You might also have observed that if we change the value of $theta_0$, i.e the intercept the line moves up and down. The intercept is important if we are interested in making predictions but not so important if we want to understand how changing X influences Y. 


### Fitting a Simple Linear Regression Model in R

We want to fit our line to a specfic set of data, we use our observed values for X and Y to estimate the values of $\theta_{0}$ and $\theta_{1}$. It is out of the scope of this workshop to examine the mathematical details of how this is done, but the objective is to draw the line that best fits the data by having the lowest total error. Here the error is defined as the difference between the observed value of Y and the predicted value of Y given X. 

In R linear regression models can be fitted with the base function `lm`. Let's look at our height and weight example. These data are available in the R object `demoDat`. In our R code we will use the formula style to specify the model we want to fit, which you may recognise from other statistical functions such as `t.test()`. The equation of the line we wish to fit needs to be provided as an argument to the `lm` function:



```{r, eval = FALSE, echo = TRUE}

lm(weight ~ height, data = demoDat)

```

The dependent variable (denoted as Y above) goes on the left of `~` and the predictor variables go on the right. The code above is specifing the model:

$$weight = \theta_0 + \theta_1 height$$

Note that we did not need to explicitly specify either the 

* intercept in our formula
or
* regression parameters

these are added in automatically. 

Let's run this bit of code

```{r, echo = TRUE}

lm(weight ~ height, data = demoDat)

```


If we execute just the `lm()` function it only prints a subset of the possible output:  

* the formula we called  
* the estimates of the coefficients.   

From these coefficients we can specify the line that has been calculated to represent the relationship between these variables.


```{r, echo = FALSE}

model <- lm(weight ~ height, data = demoDat)

```

We can see that the estimated value for the intercept is `r summary(model)$coefficients[1,1]` and the estimated value for the height slope parameter is `r summary(model)$coefficients[2,1]`. As the height parameter is positive, we can conclude that weight increases as the participants get taller. More than that we can quantify by how much. The value of the regression parameter for height tells us how much weight changes by for a one unit increase of height. To interpret this we need to know the units of our variables. In this example height is measure in cm and weight in kg, so the value of our regression coefficient mean that for each extra centimetre an individual increases by a mean of `r signif(summary(model)$coefficients[2,1],2)`kg.


```{r,echo=FALSE}
equation = paste0("$weight =",  signif(summary(model)$coefficients[1,1],3), " + ", signif(summary(model)$coefficients[2,1],3), " * Height$")
```
Therefore we can write our estimated regression model as:

`r equation`.



### Exercise 

*Let's practice fitting and interpretting the output of a simple linear regression model.*

Write the R code required to characterise how bmi changes as the participants age:

```{r ols, exercise=TRUE}

```

```{r ols-solution}
lm(bmi ~ age, data = demoDat)
```

```{r quiz1, echo=FALSE}
quiz(caption = "Quiz 1",
question("What is the value of the intercept?",
  answer("0.13", message = "This is the slope coefficient."),
  answer("1", message = "Look at the coefficients in the output."),
  answer("25", correct = TRUE),
  answer("46", message = "Did you put age and bmi in the correct way round?")
),
question("The slope coefficient is 0.13, which is the correct interpretation?",
  answer("bmi increases by 0.13 per year of age", correct = TRUE),
  answer("age increases by 0.13 year per 1 unit of bmi")
), 
question("What is the predicted BMI for a participant aged 45?",
  answer("0.9", message = "Did you forget to add the intercept?"),
  answer("25.21", message = "Did you forget to add the contribution from the slope parameter?"),
  answer("25.23", message = "Did you forget to multiple the slope parameter by the age?"),
  answer("26.1", correct = TRUE))
)
```

## Hypothesis Testing for Regression Models

If we have run regression models in other software are have seen the results of regression analysis reported in scientific reports you might be wondering where are the p-values. Don't worry they are there. Before we look at how you go about extracting this information we will first go over how hypothesis testing works in the context of regression.

### The Theory

Recall, how earlier we saw that the relationship between X and Y could be written down as a mathematical equation. 

$$Y = \theta_{0}  + \theta_{1} X$$

The important parameter for understanding the relationship between X and Y is $\theta_1$. Specifically the magnitude of $\theta_1$ tells us about the strength of the relationship between X and Y. When we looked at the different two lines you could get by changing the values of $\theta_0$ and $\theta_1$ there were two very specific scenarios we didn't consider, when $\theta_0 = 0$ or $\theta_1 = 0$. Let's see what happens to our equation in these situations:

If $\theta_0 = 0$ then our equation becomes

$$Y = 0  + \theta_{1} X = \theta_{1}$$

If we recall the intercept is the position on the y-axis when the line crosses. If $\theta_0 = 0$ then our line will cross the y-axis through the origin. 

Now, what if $\theta_1 = 0$ then our equation becomes

$$Y = \theta_0  + 0 X = \theta_{0}$$

When multiple anything by zero the answer is always 0. If the coefficient for our X variable is 0, regardless what the value of X is this will compute to 0 and we can remove it from the equation. Therefore our predictive equation for Y is no longer dependent on X.

This is essentially saying that there is no relationship between X and Y. This is the concept that underlies hypothesis testing in regression analysis. If there is a relationship between X and Y the regression parameter for X needs to be non-zero. Statistical hypothesis testing requires an explicit null hypothesis and alternative hypothesis. For each individual regression parameters these are:

$$H_{null}: \theta = 0$$
$$H_{alternative}: \theta \neq 0$$
To test these hypotheses with the observed data we implement the following steps:

1. accumulate the evidence from your data
2. use theory or your data to establish how normal/extreme this result is
3. provide some criteria to determine how to interpret the numbers 


We have already started to accumulate the evidence by estimating the regression parameters. This estimate is then converted into a t-statistic (by dividing by the estimated standard error) which enables us to use a known distribution to quantify how extreme it is. The relevant distribution here is the Student's t-distribution. With this knowledge we can then calculate a p-value which we used to decide which hypothesis our data favours. Yes, this is the same distribution as is used in the t-test, hold this thought for later.

To enable us to do this we make a series of assumptions about the data. If these do not hold true, this process potentially falls apart. Do understanding the context with which hypothesis testing has been formulated is important for derived accurate and meaningful statistical inference.

For hypothesis testing of a regression parameter for X the assumptions are:

* The dependent variable Y has a linear relationship to the independent variable X.
* Errors (residuals) are normally distributed.
* Samples are independent.
* Variances in each group are equal.

Let's look at how we do this in R

### Hypothesis Testing of Regression Parameters in R 

We don't actually need to get R to do calculations to get the p-values, it does that as default when you execute `lm()`. By default though it does not print the output to screen. We do need to use some additional commands to extract this information. This is largely acheived with the  `summary()` function. We could chain these commands together i.e. `summary(lm))` or we could save the output of `lm()` to a variable and run it as two lines of code.

```{r}
model<-lm(weight ~ height, data = demoDat)
summary(model)

```

We can see that the `summary()` function expands on the information we got from just running `lm()`.

From top to bottom, we have:

* the model formula
* a summary of the residuals (i.e errors)
* a table of coefficients
* model fit statistics. 


We are probably most interested in the coefficients table which contains a summary of each estimated regression coefficient. The results for each coefficient are provided in a separate rows, with the intercept in the top row followed by 1 row for each explanatory variable and 4 columns. It is worth noting at this point that the intercept is also a regression parameter which we can test. However, we almost never consider this result as i) it is virtually always significantly non-zero and ii) it doesn't tell us anything about any relationships between variables.  

The columns are as follows

| Column | Content |
|-------|------|
| Estimate | estimated regression coefficient | 
| Std. Error | standard error of the estimated regression coefficient |
| t value | The test statistic (the ratio of the previous two columns) | 
| Pr(>|t|) | P-value comparing the t value to the Student's t distribution | 

We can also see from the coefficients table slope parameter has a p-value < 0.05, therefore we can conclude that it is significantly non-zero, rejecting the null hypothesis in favour of the alternative hypothesis. 


We can visualise this estimated model, by generating a scatterplot of the observed data and adding the fitted regression line to the plot. 

```{r}

plot(demoDat$height, demoDat$weight, pch = 16, xlab = "Height (cm)", ylab = "Weight (kg)")
abline(model, col = "red")

```


We can see the positively correlated relationship between height and weight, and the fitted regression model appears to represent the data well. there is some noise around the fitted line. 



### Exercise 

*Let's see if age has a significant effect on weight.*

Write the R code required to test using a regression model if weight is associated with age:

```{r ols-signif, exercise=TRUE}

```

```{r ols-signif-solution}
model2<-lm(weight ~ age, data = demoDat)
summary(model2)
```

```{r quiz2, echo=FALSE}
question("Which of these statements are true? Apply a significance threshold of 0.05.",
  answer("The intercept is significantly non-zero.", correct = TRUE),
  answer("Age is significantly associated with weight.")
)
```


### Checking the Assumptions of Linear Regression

As we mentionned before, by making assumptions about the characteristics of our data we can do significance testing. It is prudent to check whether these assumptions hold true so that we can be confident in the conclusion we have derived. However, this is far from common practise. There are a number of reasons why. 

Firstly, they are subjective, so assessing is not easy especially when you are starting out. The underlying mathematics of regression is robust to some deviation from these assumptions. Common tools to assess proporties like Normality might not place the threshold for deviation in the same place as the statistical model needs. Often they are more conservative and therefore you might prematurely reject a valid result. 

Secondly, (and perhaps most truthfully) it is not possible to check many of the assumptions without fitting the model first. Therefore, you run the risk that if you find a significant effect you get excited and forgot to check the assumptions in favour of persuaing a moree exciting follow up question. However, as the assumptions are essential for deriving the formula to estimate and test the regresssion parameters, if they are violated we need to be cautious about accepting the results. 

Functionality to do this is provided within R. We can easily generate 4 plots, where we can visually inspect some of the assumptions of linear regression by applying the `plot()` function to the output of an `lm()` call. Decisions on the validity of the reults are then given by assessing these plots. Below we outline what is good and bad.

```{r}

plot(model)

```

##### Plot 1: Residuals vs Fitted

Do the residuals show a non-linear pattern?  

**Good:**   

* no evidence of a relationship between these variables. (i.e. equal number of points above and 	below the line or a random scatter)

**Bad:**   

* a pattern in the points – may be indicative of a non-linear relationship between your outcome and independent variables. 
	
##### Plot 2: Normal Q-Q

Are the residuals normally distributed?  

**Good:**  

* the points follow the diagonal line

**Bad:**  

* the points deviate from the diagonal line.

##### Plot 3: Scale-Location

Are the residuals equally spread across the range of predictions?  

**Good:**  

* horizontal line with no evidence of a relationship between these variables	

**Bad:**  

* a non horizontal line and more/less points in one corner of the plot, may indicate heteroscedasticity.
	
##### Plot 4: Residuals vs Leverage

Are any samples overly influencing the fit?  

**Good:**   

* all inside red dashed line

**Bad:**  

* any values in the upper right or lower right corner or cases outside of a dashed red line, indicates samples that don’t fit the trend in the data and are biasing the result.

As these require a subjective assessment of the plots, it can be difficult to decide whether the plot looks good or bad. This is particularly challenging where the data has only a small number of observations. We should also bare in mind that linear regression is fairly robust to the violation of many of the assumptions. Therefore, even if the plots don't look ideal, that does not automatically mean that the result is invalid. This is where statistics moves away from a fixed quantity and into a gray area of subjectivity. Experience is the only real aid here, the more regression models you fit and the more plots you look at enables you to gauge what is a major or minor deviation. 

For contrast, let's fit another linear model where the assumptions might not hold true. Let's look at the relationship between weight and hours exercised per week (`exercise_hours`). When we asked the participants how much exercise they did each week they were effectively counting the number of hours, so this variable is a count variable. Count varibles are typically poisson distributed and are whole, positive numbers, so not a continuous variable.  


```{r, echo = FALSE}
hist(demoDat$exercise_hours, xlab = "Hours exercised each week", ylab = "Number of participants")

```

In the histogram above we can see a non-symetrical distribution, a hard boundary of the left and a long tail on the right hand side. This might violate some of the assumptions if we use it as a predictor variable. Let's take a look.

```{r}
plot(lm(weight ~ exercise_hours, data  = demoDat))
```

*Plot 1* We can see clear vertical lines of dots which is a reflection of the fact the observed X variable only contains whole numbers. But otherwise the points appear reasonably random in the space, albeit more bias to the left hand side. 

*Plot 2* Points largely on the diagonal line with some deviation at the extremes.

*Plot 3* We see the vertical lines again and more points of the left hand side, but largely randomly located. 

*Plot 4* There are some samples with extreme values not quite in line the rest of the points but not outside the accepted region.  

So in conclusion we can see less desirable behaviour of our observed data as we try to force a discrete variable into a methodology for a continuous variable but the method seems capable of handling it. 



### Exercise 

*Let's have a go at interpretting some diagnostic plots of your own.*

Write the R code required to determine if a regression model is an appropriate tool for assessing the relationship between 1) age and weight and 2) bmi and alcohol units.

```{r ols-plots, exercise=TRUE}

```

```{r ols-plots-solution}
plot(lm(weight ~ age, data = demoDat))
plot(lm(bmi ~ alcohol_units, data = demoDat))
```

```{r quiz3, echo=FALSE}
question("Which of these statements are true? Apply a significance threshold of 0.05.",
  answer("The intercept is significantly non-zero.", correct = TRUE),
  answer("Age is significantly associated with weight.")
)
```


## Multiple Linear Regression 

So far we have considered a single case of regression with a continuous predictor variable and continuous outcome variable and fitting a straight line between these. This has enabled us to get to grips with the core concepts but regression is much bigger than this. It is an incredibly flexible framework that can handle different types of variables and multiple variables. We will now look at how we extend the model to incorporate more complex analysis designs.

Regression analysis is sometimes categorised into different types:

*Simple Linear Regression*

* 1 continuous outcome variable
* 1 predictor variable

*Multiple Linear Regression*

* 1 continuous outcome variable
* > 1 predictor variable

*Multivariate Linear Regression*

* multiple correlated dependent variables
* > 0 predictor variable

Here we are going to discuss *Multiple Linear Regression* which allows us to look at the effect of multiple predictor variables simultaneously. To do this we simply need to add these additional terms to our model. For example if we have two variables $X_1$ and $X_2$ and we want to use these to predict Y we can fit the model 

$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

To accomodate the extra predictor variable we need to include an extra regression parameter. We call this linear regression because we are combining the effects of these predictor variables in a linear (additive) manner.  


In R we can don't need any new functions we can fit these models with the same `lm()` function. As before R will automatically add the right number of regression parameters.

Let's look at an example where we look at the effect of both age and height on weight.

```{r}
modelM<-lm(weight ~ age + height, data = demoDat)
summary(modelM)
```


To report the results of this extra regression parameter we have a third row in the coefficient's table. The regression coefficients are tested under the same hypothesis framework we discussed for simple linear regression and the results are interpreted in the same way. From this analysis we can see that height is significantly associated with weight (p-value < 0.05) but age is not (p-value > 0.05).

All of the assumptions from simple linear regression hold for multiple linear regression with the addition of one more:

* The dependent variable Y has a linear relationship with the independent variables X.
* Errors (residuals) are normally distributed.
* Samples are independent.
* Variances in each group are equal.
* Independent variables are not highly correlated with each other.

We can again use the `plot()` function to explore these for our multiple regression model fit.

### Assessing the Effect of Multiple Predictor Variables Simultaneously

There are many reasons why you might want to model multiple predictor variables simultaneously. 

1. You are interested in understanding the effects of multiple different factors.
2. You think their are some variables that might bias or confound your analysis and you want to adjust for this. 

In second scenario some predictor variables are just included so that their effect is captured, but you are not explicitly interested in their results. 

In the first scenario, you might be interested in the quantifying the effect of each predictor term individually. This can be acheived by looking at the regression coefficients for each term in turn, as we did for the example above. Alternatively you might be interested in the combined effect of multiple terms in predicting an outcome. We can do this from our regression analysis by reframing the null and alternative hypothesis. 

Let's define our regression model for two predictor variables $X_1$ and $X_2$ as:


$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

Previously we were interested if a single regression coefficient was non-zero as if that was the case that regression parameter would cause some quantity to be added to our prediction. The null hypothesis is based on the same broad concept that there is no effect on the outcome from the predictor variables. The mathematical definition of this needs to be changed though to reflect that we have multiple predictor variables. 

If there is no effect of $X_1$ and $X_2$ on Y, then the regression coefficients for both terms will be zero. Such that they both get cancelled out of the equation and it can be simplfied to just the intercept. For there to be an improvement in the predictive capability of model only one of the two predictive variables needs to have a non-zero coefficient. 

This gives us the null hyypothesis of

$H_{null}: \theta_1 = \theta_2 = 0$

and the alternative hypothesis of 

$H_{alternative}: \theta_1 \neq 0\text{ or  }\theta_2 \neq 0$

You can think of this as comparing two models with and without the terms of interest. Our statistical question boils down to which of these models is a better fit to the data?

Model 1: 

$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

or 

Model 2: 

$$Y = \theta_0 $$

To perform this test we need to use the F-distribution rather than the T-distribution. Our test statistic is now based around the idea of variance explained. Mention of an analysis focused on the variance, and the use of the F-distribution might trigger an association with ANOVA, analysis of variance. Again hold that thought as there is a connection with regression which we will draw out later.   



### Modelling Different Types of Predictor Variables

So far we have only considered continuous predictor variables such as height and age. For linear regression is very flexibly and can handle a number of different types of variables. Next we will look at some examples so that we can understand how it works.

#### Categorical variables

We will start with categorical varibles, any scenraio where the reponses can be grouped into a finite number of categories or classes. In R this type of variable is called a factor. 

First, let's consider the simpliest type of categorical variable to model, one with only two options or groups. Sometimes this specific instance is called a binary variable. While in your data collection each group might have a text label to describe what the value is for that observation, for mathematical modelling we need to represent this binary variable numerically. This is acheived by recoding the variable such that one group is represented by 0 and one group by 1.

For example for the variable sex, we could represent females with 0 and males with 1. Note that in R if you include a factor variable in your regression model, it will automatically do this recoding for you "under the hood" without you nessescarily knowing about it. The default behaviour is for the first category alphabetically to be assigned 0 and the second category to be assigned 1. 

We the define our regression model as we did for continuous variables. For example to test for a relationship between sex and weight we can use the following code:

```{r, eval = FALSE}
lm(weight ~ sex, data = demoDat)
```

As before R will automatically add the relevant regression parameters so this model can be written mathematically as:

$$weight = \theta_0 + \theta_1 sex$$
We have our intercept regression parameter, our regression parameter for the sex variable. In this equation the variable sex takes either the value 0 or the value 1 for each observation depending if it is male or female. We also use this coding strategy to make predictions from this equation. Let's demostrate this - for males and females we can derive an equation that will represent their prediction. 

For females, $sex$ = 0. If we substitute this in we get

$$weight = \theta_0 + \theta_1 0 = \theta_0$$

Because the regression parameter for the sex variable is multiplied by 0, it will always equal 0, regardless of the value of $\theta_1$. So the prediction for females is the value of the intercept only. 

Let's do the same thing for males, where sex = 1: 

$$weight = \theta_0 + \theta_1 1 = \theta_0 + \theta_1$$
In this equation we multiple $\theta_1$ by 1 which equals $\theta_1$. So the equation for males is the intercept by the slope parameter for the sex variable. 

This helps us understand how we interpret the value of the regression coefficient for a binary variable. $\theta_1$ is the difference between the predictions for males and females so it represents the mean differents between the groups. More than that, it is not present in the equation for females but is present in the equation for males, so it specificlaly captures the difference in males relative to females. That means if it is positive, males have a higher mean than females. If it is negative males have a lower mean than females. 

The choice of how we code this variable is academic, the magnitude of the estimate will be the same, but the direction (i.e.) the sign would switch if instead we had males = 0 and females = 1.

Let's fit this model to our data:

```{r}
lm(weight ~ sex, data = demoDat)

```

We can see the estimated slope coefficient is `r signif(coef(lm(weight ~ sex, data = demoDat))[2], 4)`. It is positive so we can intepret this as males have a mean increased weight of `r signif(coef(lm(weight ~ sex, data = demoDat))[2], 4)` kg.

R tells us which group the coefficient refers to by appending to the name of the variable, the level that is coded as 1. 

But is this significant? Significance testing of regression parameters for binary variables is performed in exactly the same way. We can these results by saving the model and using the `summary()` function.

```{r}
model <- lm(weight ~ sex, data = demoDat)
summary(model)
```


If we look at the p-value column for the second row in the coefficients table, we can see that it is less < 0.05. Therefore we can reject the null hypothesis that the regression coefficient is 0 and report that based on these data, sex does have a significant effect on weight. 

Recall here that we have used the t-distribtion to test for a significant relationship between sex and weight. If you wanted to test for a difference in means between two groups, what test would you use? The t-test. If you did that here you would get an identical result.

```{r}
t.test(weight ~ sex, data = demoDat, var.equal = TRUE)
```

That is because these are mathematically equivalent, if your regression model has a continuous outcome variable and a single binary predictor variable. But of course regression is a much more flexible framework than a t-test. It can handle the inclusion of more than one variable. So if you think a t-test is what you need but you also want to be to control for other confounders, regression will allow you to do this. 


But what about categorical variables that make more than two groups? The key to include, these and indeed any other type of variable, in regression is to construct a mathematical representation.

The fundamental principle here is that for a categorical variable with m different groups, we need m-1 binary variables to parasimuously code enough unique combinations so that each group can be differentiated in the model. We saw this already for a binary categorical variables with two groups, needing the addition of one binary variable. 


Therefore, we may also want to check some of the other model summary statistics. 

In the `summary(model)` output we can see at the bottom that the results of testing the full model with an F-test. If we want to see the full table of sums of squares statistics we can use the `anova()` function on our fitted regression model.

```{r}
anova(model)

```


Comparing this table with the coefficients table, we can see that the p-value from the t-test of the age regression parameter and the F-test for the full model are identical. This is not a coincidence and is always true for the specific case of simple linear regerssion.

Finally, we will look at the $R^{2}$ and $\overline{R}^{2}$ statistics. We can see from the `summary(model)` output above these are automatically calculated. For the simple linear regression model we have fitted

$R^{2}$ = `r summary(model)$r.squared`

$\overline{R}^{2}$ = `r summary(model)$adj.r.squared`

Despite the highly significant effect of age on G, it would appear that the magnitude of the effect is small, as the proportion of variance explained is low. This fits with the scatterplot, where the points were located noisily around the fitted regression line, suggesting that predictions based soley on age are likely to be imprecise.  

#### Extracting summary statistics from a model fit in R

If you are new to R, here we will just run through some details on the type of objects these data are stored in and how to access specific elements. This can be helpful for writing automated analysis scripts. Due to the need to contain different types of data in different formats and structures, the output of the regression model fit is stored in a bespoke object, with slots for the the different parts of the output. These slots are named and can be assessed using the `$`. For example to extract just the table of estimated regression coefficients, which are named `coefficients` we use the following command: 


```{r}

summary(model)$coefficients
```

We can determine the type of object the coefficients table is stored in, using the function `class()`. 


```{r}
class(summary(model)$coefficients)
mode(summary(model)$coefficients)

```


The output of the command tells us it is stored in a matrix, which is a data-type in R, where you have rows and columns. A similar data-type is called a data.frame. The difference between these two data-types is that matrices can only contain one data type, which we can determine with the function `mode()`. Here it contains exclusively numeric values. In constrast, in a data frame each column can be a different data type. Our BDR data is stored in a data.frame and the output of the `str()` function, tells us the data type assigned to each column. 

Let's say we wanted to extract a single value from this matrix, there are a number of commands we can use. For example, let's extract the p-value for the age regression slope parameter using the slicing function `[`.

We can provide the row (2nd) and column (4th) number of the matrix entry we want: 
```{r}

summary(model)$coefficients[2,4]

```

Alternatively we can specify either the column or row name:

```{r, eval = FALSE}

summary(model)$coefficients["Age",4]

```


We can see a list of all components we can extract from the output of `lm()` by running `names()` on the lm object. All of these can be extracted with the `$`.

```{r}
names(model)
model$call ## example of how to extract any of the components listed by the previous command.
```

Similarly we can run `names()` on the `summary(lm)` object as showed here to get a list of all the slots available from that object. 

```{r}
names(summary(model))
```

Note these are different to those available for the model object, for example the $R^{2}$ and $\overline{R}^{2}$ are only extractable from the `summary(model)` object. 


```{r}
summary(model)$r.squared
summary(model)$adj.r.squared
```



Note that as well as directly assessing these slots using the `$` command, there also exist some predefined functions to extract the commonly requested outputs from the model fit. We have already taken advantage of one of these, `summary()`, others include `coef()`, `effects()`, `residuals()`, `fitted()` and `predict.lm()`.

