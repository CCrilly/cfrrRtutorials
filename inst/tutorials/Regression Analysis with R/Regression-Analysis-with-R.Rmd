---
title: "Introduction to Regression Analysis with R"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)

set.seed(123)
par(mar = c(4,4,1,1))
nSamples <- 250
height<-rnorm(nSamples, 180, 20)
sex<-as.factor(sample(c("male", "female"), size = nSamples, replace = TRUE, prob = c(0.45,0.55)))
height<-height + rnorm(nSamples, 10,5)*(as.numeric(sex)-1)
age<-floor(runif(nSamples, 40,60))
weight<- height * 0.7 - 44 + rnorm(nSamples,0,12)

weight<-weight + rnorm(nSamples, 3, 2)*(as.numeric(sex)-1) + rnorm(nSamples, 0.005, 0.001)*(as.numeric(sex)-1) * height
weight <- weight + age * rnorm(nSamples, 0.04, 0.03)
bmi <- weight/(height/100)^2

smoker<-sample(c(0,1), size = nSamples, replace = TRUE, prob = c(0.8,0.2))
t2diabetes <- sample(c(0,1), size = nSamples, replace = TRUE, prob = c(0.8,0.2))
t2diabetes[sample(which(bmi > 25),10)]<-1
t2diabetes[sample(which(smoker == 1),5)]<-1

exercise_hours <- rpois(nSamples, 1) + rpois(nSamples, 2)*(1-t2diabetes) + rpois(nSamples, 1) * (as.numeric(sex)-1)
alcohol_units <- rpois(nSamples, 3) + rpois(nSamples, 5)*(1-t2diabetes) + rpois(nSamples, 3) * (as.numeric(sex)-1) + rpois(nSamples, 1)*rpois(nSamples, 6)*(1-t2diabetes) 
alcohol_units[which(bmi > 37)]<-alcohol_units[which(bmi > 37)] + rpois(sum(bmi > 37),5)
alcohol_units[which(weight > 140)]<-rpois(sum(weight > 140),50)

ethnicity<-sample(c("European", "Asian", "AfricanAmerican"), nSamples, replace = TRUE, prob = c(0.6,0.25,0.15))
socioeconomic_status <- sample(c("High", "Middle", "Low"), nSamples, replace = TRUE, prob = c(0.25,0.5,0.25))
socioeconomic_status[which(bmi > 25)] <- sample(c("High", "Middle", "Low"), sum(bmi > 25), replace = TRUE, prob = c(0.1,0.25,0.65))

demoDat <-data.frame(age, height, weight, bmi, ethnicity, socioeconomic_status, smoker, exercise_hours, alcohol_units, t2diabetes)

```

## Overview of Workshop

Welcome to Introductory Regression Analysis with R. Our aim is to provide you with a comprehensive introduction to the statistical tool regression. In this session you will learn what regression is, how to fit a range of regression models with R, how to interpret the output and the link between regression and other common statistical tools.

## Introduction to Regression

### What is Regression?

Regression analysis is a broad category of analyses where the objective is to statistically quantify relationships between variables.

It enables you to:

* understand which variables affect other variables and how
* make predictions from a new set of data

You might also know it as fitting a model to data. Where model is a mathematical specification of the relationship between the variables and we use our data to determine what numbers the model should include. 

It requires:

* dependent variable(s) - the outcome or variable you are trying to predict
* independent variable(s) - the predictors, features, covariates or explanatory variable(s)

You may also know it as fitting a line to data as in the example below, where the line is a graphical representation of a model. Note by line we are not limited to just a straight line.

```{r straight line example, echo = FALSE, fig.height= 4, fig.width=4}
set.seed(123)
par(mar = c(4,4,1,1))
nSamples <- 25
height<-rnorm(nSamples, 180, 20)
weight<- height * 0.62 - 44 + rnorm(nSamples,0,5)

plot(height, weight, pch = 16, col = , xlab = "Height (cm)", ylab = "Weight (kg)")
abline(a = -44, b = 0.62)
```


### What is a line?

To understand a bit more about regression parameters we are going to recap what a line, specifically a straight line, is in a mathematical sense. If we have two continuous variables such as height and weight, measured on the same set of individuals we can visualise the relationship with a scatterplot like the one above and try to generalise it by drawing a straight line through the points. From this line we can make predictions of what weight a person might have if we know their height. 

To do this we need to represent this relationship as an equation. The standard equation for a straight line between two variables Y and X:

$$Y = \theta_{0}  + \theta_{1} X$$

There are two regression parameters in this equation: 

1. Intercept ($\theta_{0}$ ) - This is the value of the outcome variable when the predictor is set to 0. 
2. Slope coefficient ($\theta_{1}$) - This is the change in the outcome variable for each unit of the predictor variable.

When we know the values of these parameters we can then input different values of X to make predictions for Y. What's more hanging the values of these parameters changes how the line appears. 

```{r straight line multiple, echo = FALSE, fig.height= 6, fig.width=6}
par(mar = c(4,4,1,1))
par(mfrow = c(2,2))

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + 5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 10, b = 5)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + -5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 10, b = -5)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 10 + 10X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 5, b = 10)
abline(v = 0)
abline(h = 0)

plot(0, 1, type = "n", col = , xlab = "X", ylab = "Y", main = "Y = 5 + 5X", xlim = c(-10,10), ylim = c(-10,10))
abline(a = 5, b = 5)
abline(v = 0)
abline(h = 0)
```

Above we showcase a number of different lines. What you should observe is that  changing $theta_{1}$ changes the slope of the line. We can change the direction of the line depending if the parameter is negative or positive, we can change the steepness of the slope changing the magnitude of this parameter. This is the parameter that captures the relationship between our variables X and Y and enables us to model infinite linear relationships.  

You might also have observed that if we change the value of $theta_0$, i.e the intercept the line moves up and down. The intercept is important if we are interested in making predictions but not so important if we want to understand how changing X influences Y. 


### Fitting a Simple Linear Regression Model in R

We want to fit our line to a specfic set of data, we use our observed values for X and Y to estimate the values of $\theta_{0}$ and $\theta_{1}$. It is out of the scope of this workshop to examine the mathematical details of how this is done, but the objective is to draw the line that best fits the data by having the lowest total error. Here the error is defined as the difference between the observed value of Y and the predicted value of Y given X. 

In R linear regression models can be fitted with the base function `lm`. Let's look at our height and weight example. These data are available in the R object `demoDat`. In our R code we will use the formula style to specify the model we want to fit, which you may recognise from other statistical functions such as `t.test()`. The equation of the line we wish to fit needs to be provided as an argument to the `lm` function:



```{r, eval = FALSE, echo = TRUE}

lm(weight ~ height, data = demoDat)

```

The dependent variable (denoted as Y above) goes on the left of `~` and the predictor variables go on the right. The code above is specifing the model:

$$weight = \theta_0 + \theta_1 height$$

Note that we did not need to explicitly specify either the 

* intercept in our formula
or
* regression parameters

these are added in automatically. 

Let's run this bit of code

```{r, echo = TRUE}

lm(weight ~ height, data = demoDat)

```


If we execute just the `lm()` function it only prints a subset of the possible output:  

* the formula we called  
* the estimates of the coefficients.   

From these coefficients we can specify the line that has been calculated to represent the relationship between these variables.


```{r, echo = FALSE}

model <- lm(weight ~ height, data = demoDat)

```

We can see that the estimated value for the intercept is `r summary(model)$coefficients[1,1]` and the estimated value for the height slope parameter is `r summary(model)$coefficients[2,1]`. As the height parameter is positive, we can conclude that weight increases as the participants get taller. More than that we can quantify by how much. The value of the regression parameter for height tells us how much weight changes by for a one unit increase of height. To interpret this we need to know the units of our variables. In this example height is measure in cm and weight in kg, so the value of our regression coefficient mean that for each extra centimetre an individual increases by a mean of `r signif(summary(model)$coefficients[2,1],2)`kg.


```{r,echo=FALSE}
equation = paste0("$weight =",  signif(summary(model)$coefficients[1,1],3), " + ", signif(summary(model)$coefficients[2,1],3), " * Height$")
```
Therefore we can write our estimated regression model as:

`r equation`.



### Exercise 

*Let's practice fitting and interpretting the output of a simple linear regression model.*

Write the R code required to characterise how bmi changes as the participants age:

```{r ols, exercise=TRUE}

```

```{r ols-solution}
lm(bmi ~ age, data = demoDat)
```

```{r quiz1, echo=FALSE}
quiz(caption = "Quiz 1",
question("What is the value of the intercept?",
  answer("0.13", message = "This is the slope coefficient."),
  answer("1", message = "Look at the coefficients in the output."),
  answer("25", correct = TRUE),
  answer("46", message = "Did you put age and bmi in the correct way round?")
),
question("The slope coefficient is 0.13, which is the correct interpretation?",
  answer("bmi increases by 0.13 per year of age", correct = TRUE),
  answer("age increases by 0.13 year per 1 unit of bmi")
), 
question("What is the predicted BMI for a participant aged 45?",
  answer("0.9", message = "Did you forget to add the intercept?"),
  answer("25.21", message = "Did you forget to add the contribution from the slope parameter?"),
  answer("25.23", message = "Did you forget to multiple the slope parameter by the age?"),
  answer("26.1", correct = TRUE))
)
```

## Hypothesis Testing for Regression Models

If we have run regression models in other software are have seen the results of regression analysis reported in scientific reports you might be wondering where are the p-values. Don't worry they are there. Before we look at how you go about extracting this information we will first go over how hypothesis testing works in the context of regression.

### The Theory

Recall, how earlier we saw that the relationship between X and Y could be written down as a mathematical equation. 

$$Y = \theta_{0}  + \theta_{1} X$$

The important parameter for understanding the relationship between X and Y is $\theta_1$. Specifically the magnitude of $\theta_1$ tells us about the strength of the relationship between X and Y. When we looked at the different two lines you could get by changing the values of $\theta_0$ and $\theta_1$ there were two very specific scenarios we didn't consider, when $\theta_0 = 0$ or $\theta_1 = 0$. Let's see what happens to our equation in these situations:

If $\theta_0 = 0$ then our equation becomes

$$Y = 0  + \theta_{1} X = \theta_{1}$$

If we recall the intercept is the position on the y-axis when the line crosses. If $\theta_0 = 0$ then our line will cross the y-axis through the origin. 

Now, what if $\theta_1 = 0$ then our equation becomes

$$Y = \theta_0  + 0 X = \theta_{0}$$

When multiple anything by zero the answer is always 0. If the coefficient for our X variable is 0, regardless what the value of X is this will compute to 0 and we can remove it from the equation. Therefore our predictive equation for Y is no longer dependent on X.

This is essentially saying that there is no relationship between X and Y. This is the concept that underlies hypothesis testing in regression analysis. If there is a relationship between X and Y the regression parameter for X needs to be non-zero. Statistical hypothesis testing requires an explicit null hypothesis and alternative hypothesis. For each individual regression parameters these are:

$$H_{null}: \theta = 0$$
$$H_{alternative}: \theta \neq 0$$
To test these hypotheses with the observed data we implement the following steps:

1. accumulate the evidence from your data
2. use theory or your data to establish how normal/extreme this result is
3. provide some criteria to determine how to interpret the numbers 


We have already started to accumulate the evidence by estimating the regression parameters. This estimate is then converted into a t-statistic (by dividing by the estimated standard error) which enables us to use a known distribution to quantify how extreme it is. The relevant distribution here is the Student's t-distribution. With this knowledge we can then calculate a p-value which we used to decide which hypothesis our data favours. Yes, this is the same distribution as is used in the t-test, hold this thought for later.

To enable us to do this we make a series of assumptions about the data. If these do not hold true, this process potentially falls apart. Do understanding the context with which hypothesis testing has been formulated is important for derived accurate and meaningful statistical inference.

For hypothesis testing of a regression parameter for X the assumptions are:

* The dependent variable Y has a linear relationship to the independent variable X.
* Errors (residuals) are normally distributed.
* Samples are independent.
* Variances in each group are equal.

Let's look at how we do this in R

### Hypothesis Testing of Regression Parameters in R 

We don't actually need to get R to do calculations to get the p-values, it does that as default when you execute `lm()`. By default though it does not print the output to screen. We do need to use some additional commands to extract this information. This is largely acheived with the  `summary()` function. We could chain these commands together i.e. `summary(lm))` or we could save the output of `lm()` to a variable and run it as two lines of code.

```{r}
model<-lm(weight ~ height, data = demoDat)
summary(model)

```

We can see that the `summary()` function expands on the information we got from just running `lm()`.

From top to bottom, we have:

* the model formula
* a summary of the residuals (i.e errors)
* a table of coefficients
* model fit statistics. 


We are probably most interested in the coefficients table which contains a summary of each estimated regression coefficient. The results for each coefficient are provided in a separate rows, with the intercept in the top row followed by 1 row for each explanatory variable and 4 columns. It is worth noting at this point that the intercept is also a regression parameter which we can test. However, we almost never consider this result as i) it is virtually always significantly non-zero and ii) it doesn't tell us anything about any relationships between variables.  

The columns are as follows

| Column | Content |
|-------|------|
| Estimate | estimated regression coefficient | 
| Std. Error | standard error of the estimated regression coefficient |
| t value | The test statistic (the ratio of the previous two columns) | 
| Pr(>|t|) | P-value comparing the t value to the Student's t distribution | 

We can also see from the coefficients table slope parameter has a p-value < 0.05, therefore we can conclude that it is significantly non-zero, rejecting the null hypothesis in favour of the alternative hypothesis. 


We can visualise this estimated model, by generating a scatterplot of the observed data and adding the fitted regression line to the plot. 

```{r}

plot(demoDat$height, demoDat$weight, pch = 16, xlab = "Height (cm)", ylab = "Weight (kg)")
abline(model, col = "red")

```


We can see the positively correlated relationship between height and weight, and the fitted regression model appears to represent the data well. there is some noise around the fitted line. 



### Exercise 

*Let's see if age has a significant effect on weight.*

Write the R code required to test using a regression model if weight is associated with age:

```{r ols-signif, exercise=TRUE}

```

```{r ols-signif-solution}
model2<-lm(weight ~ age, data = demoDat)
summary(model2)
```

```{r quiz2, echo=FALSE}
question("Which of these statements are true? Apply a significance threshold of 0.05.",
  answer("The intercept is significantly non-zero.", correct = TRUE),
  answer("Age is significantly associated with weight.")
)
```


### Checking the Assumptions of Linear Regression

As we mentionned before, by making assumptions about the characteristics of our data we can do significance testing. It is prudent to check whether these assumptions hold true so that we can be confident in the conclusion we have derived. However, this is far from common practise. There are a number of reasons why. 

Firstly, they are subjective, so assessing is not easy especially when you are starting out. The underlying mathematics of regression is robust to some deviation from these assumptions. Common tools to assess proporties like Normality might not place the threshold for deviation in the same place as the statistical model needs. Often they are more conservative and therefore you might prematurely reject a valid result. 

Secondly, (and perhaps most truthfully) it is not possible to check many of the assumptions without fitting the model first. Therefore, you run the risk that if you find a significant effect you get excited and forgot to check the assumptions in favour of persuaing a moree exciting follow up question. However, as the assumptions are essential for deriving the formula to estimate and test the regresssion parameters, if they are violated we need to be cautious about accepting the results. 

Functionality to do this is provided within R. We can easily generate 4 plots, where we can visually inspect some of the assumptions of linear regression by applying the `plot()` function to the output of an `lm()` call. Decisions on the validity of the reults are then given by assessing these plots. Below we outline what is good and bad.

```{r}

plot(model)

```

##### Plot 1: Residuals vs Fitted

Do the residuals show a non-linear pattern?  

**Good:**   

* no evidence of a relationship between these variables. (i.e. equal number of points above and 	below the line or a random scatter)

**Bad:**   

* a pattern in the points – may be indicative of a non-linear relationship between your outcome and independent variables. 
	
##### Plot 2: Normal Q-Q

Are the residuals normally distributed?  

**Good:**  

* the points follow the diagonal line

**Bad:**  

* the points deviate from the diagonal line.

##### Plot 3: Scale-Location

Are the residuals equally spread across the range of predictions?  

**Good:**  

* horizontal line with no evidence of a relationship between these variables	

**Bad:**  

* a non horizontal line and more/less points in one corner of the plot, may indicate heteroscedasticity.
	
##### Plot 4: Residuals vs Leverage

Are any samples overly influencing the fit?  

**Good:**   

* all inside red dashed line

**Bad:**  

* any values in the upper right or lower right corner or cases outside of a dashed red line, indicates samples that don’t fit the trend in the data and are biasing the result.

As these require a subjective assessment of the plots, it can be difficult to decide whether the plot looks good or bad. This is particularly challenging where the data has only a small number of observations. We should also bare in mind that linear regression is fairly robust to the violation of many of the assumptions. Therefore, even if the plots don't look ideal, that does not automatically mean that the result is invalid. This is where statistics moves away from a fixed quantity and into a gray area of subjectivity. Experience is the only real aid here, the more regression models you fit and the more plots you look at enables you to gauge what is a major or minor deviation. 

For contrast, let's fit another linear model where the assumptions might not hold true. Let's look at the relationship between weight and hours exercised per week (`exercise_hours`). When we asked the participants how much exercise they did each week they were effectively counting the number of hours, so this variable is a count variable. Count varibles are typically poisson distributed and are whole, positive numbers, so not a continuous variable.  


```{r, echo = FALSE}
hist(demoDat$exercise_hours, xlab = "Hours exercised each week", ylab = "Number of participants")

```

In the histogram above we can see a non-symetrical distribution, a hard boundary of the left and a long tail on the right hand side. This might violate some of the assumptions if we use it as a predictor variable. Let's take a look.

```{r}
plot(lm(weight ~ exercise_hours, data  = demoDat))
```

*Plot 1* We can see clear vertical lines of dots which is a reflection of the fact the observed X variable only contains whole numbers. But otherwise the points appear reasonably random in the space, albeit more bias to the left hand side. 

*Plot 2* Points largely on the diagonal line with some deviation at the extremes.

*Plot 3* We see the vertical lines again and more points of the left hand side, but largely randomly located. 

*Plot 4* There are some samples with extreme values not quite in line the rest of the points but not outside the accepted region.  

So in conclusion we can see less desirable behaviour of our observed data as we try to force a discrete variable into a methodology for a continuous variable but the method seems capable of handling it. 



### Exercise 

*Let's have a go at interpretting some diagnostic plots of your own.*

Write the R code required to determine if a regression model is an appropriate tool for assessing the relationship between 1) age and weight and 2) bmi and alcohol units.

```{r ols-plots, exercise=TRUE}

```

```{r ols-plots-solution}
plot(lm(weight ~ age, data = demoDat))
plot(lm(bmi ~ alcohol_units, data = demoDat))
```

```{r quiz2b, echo=FALSE}
question("Which of these statements are true? Apply a significance threshold of 0.05.",
  answer("The intercept is significantly non-zero.", correct = TRUE),
  answer("Age is significantly associated with weight.")
)
```


## Multiple Linear Regression 

So far we have considered a single case of regression with a continuous predictor variable and continuous outcome variable and fitting a straight line between these. This has enabled us to get to grips with the core concepts but regression is much bigger than this. It is an incredibly flexible framework that can handle different types of variables and multiple variables. We will now look at how we extend the model to incorporate more complex analysis designs.

Regression analysis is sometimes categorised into different types:

*Simple Linear Regression*

* 1 continuous outcome variable
* 1 predictor variable

*Multiple Linear Regression*

* 1 continuous outcome variable
* > 1 predictor variable

*Multivariate Linear Regression*

* multiple correlated dependent variables
* > 0 predictor variable

Here we are going to discuss *Multiple Linear Regression* which allows us to look at the effect of multiple predictor variables simultaneously. To do this we simply need to add these additional terms to our model. For example if we have two variables $X_1$ and $X_2$ and we want to use these to predict Y we can fit the model 

$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

To accomodate the extra predictor variable we need to include an extra regression parameter. We call this linear regression because we are combining the effects of these predictor variables in a linear (additive) manner.  


In R we can don't need any new functions we can fit these models with the same `lm()` function. As before R will automatically add the right number of regression parameters.

Let's look at an example where we look at the effect of both age and height on weight.

```{r}
modelM<-lm(weight ~ age + height, data = demoDat)
summary(modelM)
```


To report the results of this extra regression parameter we have a third row in the coefficient's table. The regression coefficients are tested under the same hypothesis framework we discussed for simple linear regression and the results are interpreted in the same way. From this analysis we can see that height is significantly associated with weight (p-value < 0.05) but age is not (p-value > 0.05).

All of the assumptions from simple linear regression hold for multiple linear regression with the addition of one more:

* The dependent variable Y has a linear relationship with the independent variables X.
* Errors (residuals) are normally distributed.
* Samples are independent.
* Variances in each group are equal.
* Independent variables are not highly correlated with each other.

We can again use the `plot()` function to explore these for our multiple regression model fit.

### Assessing the Effect of Multiple Predictor Variables Simultaneously

There are many reasons why you might want to model multiple predictor variables simultaneously. 

1. You are interested in understanding the effects of multiple different factors.
2. You think their are some variables that might bias or confound your analysis and you want to adjust for this. 

In second scenario some predictor variables are just included so that their effect is captured, but you are not explicitly interested in their results. 

In the first scenario, you might be interested in the quantifying the effect of each predictor term individually. This can be acheived by looking at the regression coefficients for each term in turn, as we did for the example above. Alternatively you might be interested in the combined effect of multiple terms in predicting an outcome. We can do this from our regression analysis by reframing the null and alternative hypothesis. 

Let's define our regression model for two predictor variables $X_1$ and $X_2$ as:


$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

Previously we were interested if a single regression coefficient was non-zero as if that was the case that regression parameter would cause some quantity to be added to our prediction. The null hypothesis is based on the same broad concept that there is no effect on the outcome from the predictor variables. The mathematical definition of this needs to be changed though to reflect that we have multiple predictor variables. 

If there is no effect of $X_1$ and $X_2$ on Y, then the regression coefficients for both terms will be zero. Such that they both get cancelled out of the equation and it can be simplfied to just the intercept. For there to be an improvement in the predictive capability of model only one of the two predictive variables needs to have a non-zero coefficient. 

This gives us the null hyypothesis of

$H_{null}: \theta_1 = \theta_2 = 0$

and the alternative hypothesis of 

$H_{alternative}: \theta_1 \neq 0\text{ or  }\theta_2 \neq 0$

You can think of this as comparing two models with and without the terms of interest. Our statistical question boils down to which of these models is a better fit to the data?

Model 1: 

$$Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 $$

or 

Model 2: 

$$Y = \theta_0 $$

To perform this test we need to use the F-distribution rather than the T-distribution. Our test statistic is now based around the idea of variance explained. Mention of an analysis focused on the variance, and the use of the F-distribution might trigger an association with ANOVA, analysis of variance. Again hold that thought as there is a connection with regression which we will draw out later.   



### Modelling Different Types of Predictor Variables

So far we have only considered continuous predictor variables such as height and age. For linear regression is very flexibly and can handle a number of different types of variables. Next we will look at some examples so that we can understand how it works.

#### Categorical variables

We will start with categorical varibles, any scenraio where the reponses can be grouped into a finite number of categories or classes. In R this type of variable is called a factor. 

First, let's consider the simpliest type of categorical variable to model, one with only two options or groups. Sometimes this specific instance is called a binary variable. While in your data collection each group might have a text label to describe what the value is for that observation, for mathematical modelling we need to represent this binary variable numerically. This is acheived by recoding the variable such that one group is represented by 0 and one group by 1.

For example for the variable sex, we could represent females with 0 and males with 1. Note that in R if you include a factor variable in your regression model, it will automatically do this recoding for you "under the hood" without you nessescarily knowing about it. The default behaviour is for the first category alphabetically to be assigned 0 and the second category to be assigned 1. 

We the define our regression model as we did for continuous variables. For example to test for a relationship between sex and weight we can use the following code:

```{r, eval = FALSE}
lm(weight ~ sex, data = demoDat)
```

As before R will automatically add the relevant regression parameters so this model can be written mathematically as:

$$weight = \theta_0 + \theta_1 sex$$
We have our intercept regression parameter, our regression parameter for the sex variable. In this equation the variable sex takes either the value 0 or the value 1 for each observation depending if it is male or female. We also use this coding strategy to make predictions from this equation. Let's demostrate this - for males and females we can derive an equation that will represent their prediction. 

For females, $sex$ = 0. If we substitute this in we get

$$weight = \theta_0 + \theta_1 0 = \theta_0$$

Because the regression parameter for the sex variable is multiplied by 0, it will always equal 0, regardless of the value of $\theta_1$. So the prediction for females is the value of the intercept only. 

Let's do the same thing for males, where sex = 1: 

$$weight = \theta_0 + \theta_1 1 = \theta_0 + \theta_1$$
In this equation we multiple $\theta_1$ by 1 which equals $\theta_1$. So the equation for males is the intercept by the slope parameter for the sex variable. 

This helps us understand how we interpret the value of the regression coefficient for a binary variable. $\theta_1$ is the difference between the predictions for males and females so it represents the mean differents between the groups. More than that, it is not present in the equation for females but is present in the equation for males, so it specificlaly captures the difference in males relative to females. That means if it is positive, males have a higher mean than females. If it is negative males have a lower mean than females. 

The choice of how we code this variable is academic, the magnitude of the estimate will be the same, but the direction (i.e.) the sign would switch if instead we had males = 0 and females = 1.

Let's fit this model to our data:

```{r}
lm(weight ~ sex, data = demoDat)

```

We can see the estimated slope coefficient is `r signif(coef(lm(weight ~ sex, data = demoDat))[2], 4)`. It is positive so we can intepret this as males have a mean increased weight of `r signif(coef(lm(weight ~ sex, data = demoDat))[2], 4)` kg.

R tells us which group the coefficient refers to by appending to the name of the variable, the level that is coded as 1. 

But is this significant? Significance testing of regression parameters for binary variables is performed in exactly the same way. We can these results by saving the model and using the `summary()` function.

```{r}
model <- lm(weight ~ sex, data = demoDat)
summary(model)
```


If we look at the p-value column for the second row in the coefficients table, we can see that it is less < 0.05. Therefore we can reject the null hypothesis that the regression coefficient is 0 and report that based on these data, sex does have a significant effect on weight. 

Recall here that we have used the t-distribtion to test for a significant relationship between sex and weight. If you wanted to test for a difference in means between two groups, what test would you use? The t-test. If you did that here you would get an identical result.

```{r}
t.test(weight ~ sex, data = demoDat, var.equal = TRUE)
```

That is because these are mathematically equivalent, if your regression model has a continuous outcome variable and a single binary predictor variable. But of course regression is a much more flexible framework than a t-test. It can handle the inclusion of more than one variable. So if you think a t-test is what you need but you also want to be to control for other confounders, regression will allow you to do this. 


But what about categorical variables that make more than two groups? The key to include, these and indeed any other type of variable, in regression is to construct a mathematical representation.

The fundamental principle here is that for a categorical variable with m different groups, we need m-1 binary variables to parasimuously code enough unique combinations so that each group can be differentiated in the model. We saw this already for a binary categorical variables with two groups, needing the addition of one binary variable. 

Let's consider a slightly more complicated example - a categorical variable with three options for example ethnicity. To represent this numerically as a series of dummy variables we would need two binary variables. The table below shows how for each of the three options, across these two binary variables ("Asian" and "European"), we can code for any of the three variables. We could have a third variable ("African American") but this would be redundant.


| Eye Colour      | Asian | European |
| ----------- | ----------- | ----------- |
|  African American      | 0       | 0 |
| Asian   | 1        | 0 |
| European | 0 | 1 |


To add this three level factor into our model we need to add both of these dummy variables. Each variable will also then need it's own regression parameter. So if we were interested in the effect on weight of ethnicty our regression model would look like this:

$$weight = \theta_0 + \theta_1 Asian + \theta_2 European$$

As before when we specify the model we want to fit in R, we don't need to explicit include the regression parameters. In a similar way, we don't need to manually add the relevant dummy variables, if you tell R to fit a regression model with a categorical variable, it will automatically add the relevant number of dummy variables.

For this example the following code is sufficient

```{r}
model <- lm(weight ~ ethnicity, data = demoDat)
summary(model)
```

We can see from the coefficients table in the output, there are three rows for the three regression parameters: the intercept and the two slope parameters for the two dummy variables. 

As these are dummy variables, the interpretation of the estimated effect (i.e. regression coefficient) for each of these is the same as for the binary variable. It cpatures the mean difference for that group relative to the baseline or reference group. The `ethnicityAsian` row is estimating the mean difference between Asians and African Americans, while the `ethnicityEuropean` row is estimating the mean difference between the Europeans and African Americans. In this model it is also worth remembering that the intercept row, gives you the mean value of the outcome (i.e. weight) of the reference group, which is the African Americians. By adding on the value of either regression parameter we can get the mean value of the other two ethic groups. 

```{r multipleRegression, exercise=TRUE}
# Fit a linear regression model to test for differences in weight by socioeconomic class.



```


```{r multipleRegression-solution}
summary(lm(weight ~ socioeconomic_status, data = demoDat))
```


```{r quiz3, echo=FALSE}
quiz(caption = "Multiple Regression Quiz Part A",
  question("What is the reference group in this example?",
    answer("Low"),
    answer("Middle"),
    answer("High", correct = TRUE)
  ),
  question("What does the intercept represent?",
    answer("The mean weigth in the low group."),
    answer("The mean weigth in the middle group."),
    answer("The mean weigth in the high group.", correct = TRUE)
  ),
  question("What is the mean weight of the low income group?",
           answer("7.49", message = "This is slope coefficient for the low group. What is it's intepretation?"),
           answer("7.87", message = "This is the regression coefficient for the Medium Group. Re-read the question."),
           answer("82.9", message = "Corrext process, but you've choosen the wrong regression coefficient."),
           answer("90.4", correct = TRUE)
)
)
```


We can do significance testing for each dummy variable in the same way as we have done up to this point. Determinging if the regression coefficient is non-zero means that that term significanctly influence the regression model and a relthionship between that and the outcome can be concluded. In the example above etween weigth and scoicoeconomic group, both the p-values for both dummy variables are < 0.05. Therefore we can conclude that there is a significant difference in the mean between the high socioeconomic group and the low socioeconomic groups AND there is a significant difference in the mean weight between the mioddle and high socioeconomic groups. In this situation it is fairly straighforward to draw a conclusion.   

But what if our two or more p-values show difference results? What;s more, the interpretation of significance from this testing framework is slightly more nuanced than perhaps we were interested in. Our question might be broader than knowing about the significance of specific comparisions. 

Instead, our question might be framed as is there a difference between any of these groups? In this situation we are interesting in whether any of the dummary variables have an effect on the model. We have slightly different null hyothesis for this statistical test.

For the regression model

$$weight = \theta_0 + \theta_1 Middle + \theta_2 High$$

if there was no relathionship between sociecnomic status and weight then niether regression parameter for either dummy variable would be non-zero. To remove them fromthe prediction equation both need to be equal to zero, which gives us the null hypothesis of:

$$H_{null}: \theta_1 = \theta_2 = 0$$
For there to be any beneifit to the prediction equation of weight by knwoing socioeconomic status, then we would need one of these slop parameters to be non-zero. It could be either of them or it could be both of them. For the purposes of determining a significant relationship, betwwen the cetegorical variable and outcome, we don't care. 

So our alternative hypothesis becomes

$$H_{alternative}: \text{there exists } \theta_i \neq 0,\text{ for i  = 1,2.}$$



For this statistical test, we use the F distribution. The code we need to run is:

```{r}
anova(lm(weight ~ socioeconomic_status, data = demoDat))
```


We can see in the output a table of test statistics where we can see an F-value is reported along with it's p-value. In this example we have a non-significant p-value, that is just greater than 0.05.


This approach can also be thought of as testing these two models:

Model 1:

$$weight = \theta_0$$
Model 2:

$$weight = \theta_0 + \theta_1 Middle + \theta_2 High$$

And asking whether Model 2 (the more complex model) is a significantly better predictor that Model 1 (which we sometimes refer to as the null model).

We can code this statistical comparision in R as follows:

```{r}
model.null <- lm(weight ~ 1, data = demoDat)
model.full <- lm(weight ~ socioeconomic_status, data = demoDat)
anova(model.null, model.full)

```


We can see in the output, it first confirms the two models we are comparing. Second, it provides a table of test statistics where we can see an F-statistic is reported along with it's p-value. The result is identical to directly applying the anova function to the full model.

We can extend this principle of testing the joint effect of multiple dummy variable to test for joint effects of any set of multiple predictor variables. For example we could test if the addition of age and height together improves the model:

```{r}
model.full <- lm(weight ~ age + height, data = demoDat)
model.null <- lm(weight ~ 1, data = demoDat)
anova(model.null, model.full)

```

In fact, the null model does not have to be as simple as including just the intercept. It just needs to contain a subset of the variables in the full model. This can be a really powerful strtegy if you have soem variables you want to adjust for but aren't interested in their relationsip onthe outcome. Maybe inour example of scoieconomic status, we know that age and height might confound the effect of socioeconomic status. So in our significance testing we want to be able to adjust for them while evaluatinf the effect of socioeconomic status.  We can do this as follows:

```{r}
model.null <- lm(weight ~ age + height, data = demoDat)
model.full <- lm(weight ~ socioeconomic_status + age + height, data = demoDat)
anova(model.null, model.full)
```

We can see here, that having adjusted for the potential confounders we now have a significant relationship between socioeconomic status and weight. 


```{r multipleRegression2, exercise=TRUE}
# Use an F-test to evaluate the relationship between BMI and ethnicity.


```

```{r multipleRegression2-solution, exercise=TRUE}
# Use an F-test to evaluate the relationship between BMI and ethnicity.
model<-lm(bmi ~ ethnicity, data = demoDat)
null <- lm(bmi ~ 1, data = demoDat)
anova(null, model)

```


```{r quiz3b, echo=FALSE}
quiz(
question("Which of these R codes is considered the null model in this example?",
  answer("$bmi ~ 1$", correct = TRUE),
  answer("$bmi ~ ethnicity$")),
question("How many regression parameters are estimated in the null model?",
  answer("0"),
  answer("1", correct = TRUE, message = "1 for the intercept."),
  answer("2"),
  answer("3")
  ),
question("How many regression parameters are estimated in the full model?",
  answer("0"),
  answer("1"),
  answer("2"),
  answer("3", correct = TRUE, message = "1 for the intercept and 2 for the dummy variables for ethnicity.")
  ),
question("Is there a significant relationship between ethnicity and bmi?",
  answer("P-value is > 0.05. So no significant relationship.", correct = TRUE),
  answer("P-value is < 0.05. So no significant relationship."),
  answer("P-value is > 0.05. So significant relationship."),
  answer("P-value is < 0.05. So significant relationship.")
  )
)
```


## Logistic Regression

So far all the examples have assumed we have a continous outcome we want to predict. But this is unlikely to be the case all of the time. What if we want to predict a binary variable, such as case control status or another type of indicator variable. We can't use the traditional linear regression techniques we have discussed so far because our outcome is now discrete (coded 0 or 1) but our linear combination of predictor variables can take value. We need to use a function to translate this continous value into our 0,1 discrete code. 

To do this we use the logitisc function, and hence this class of regression models is called logitstic regression. 

The logistic function is defined as 

$$\sigma(t) = \frac{e^t}{e^t+1}$$

We can visualise the transform between the right hand side and the left hand side of the equation in the graph below

```{r, echo = FALSE}
xseq <- seq(-6,6, 0.1)
yseq <- exp(xseq)/(exp(xseq)+1)

plot(xseq, yseq,type = "line", xlab = "X", ylab = "Y", lwd = 1.5)
abline(h = 0.5, lty = 2)
abline(h = 1)
abline(h = 0)

```


While the combination of X variables can take any value from - infinity to infinity, the Y value is constrained to between 0 and 1. It is still a continous function, so we haven't quite got to our desired outcome of a binary 0,1 variable. With Y now constrained to fall between 0 and 1 we can interpet it as a probability, the probability of being a case. To transform this to taken either the value 0 or the value 1, we apply a threshold of 0.5. If the predicted Y > 0.5 then Y is that observation is classed as a case (i.e. Y = 1) whereaseif predicted Y < 0.5, then that observation is classed as a control (i.e. Y = 0). In an ideal world we want our predictions to be definitive. So as well as being constrained to giving a value between 0 and 1, the transformation also has the desirable property of spending most of it's time at the extremes (i.e. 0 or 1) and not much time in the middle (i.e. around 0.5).

After applying the link function, technically, logistic regression is estimating the log odds of being a case. So our equation becomes 

$ln(odds) = ln(\frac{p}{(1-p)}) = \beta_0 + \beta_1*x$.

We are no longer in the class of linear regression, we are in a more general class of generalised linear models. These permit a more varied number of regression models with different types of outcomes.They use a link function to transform form the unbounded prediction on the right hand side to the properties of the outcome variable on the left hand side. For logistic regression we the link function in the logistic function. This means we also need a new R function, `glm()` to fit them. 

Let's look at an example we are going to predict Type II diabetes status from bmi.

```{r}
model.log <- glm(t2diabetes ~ bmi, data = demoDat, family = "binomial")

summary(model.log)
```

The output takes a very similar format to the `lm()` output. What differences can you notice?

* Instead of residuals we have deviance residuals.

* Instead of t-statistics we have z-statistics.

* Instead of the sums of squares statistics and F-test, we have deviance statistics and no automatic overall test for the full model.

#### Interpretation of Regression Coefficients

Remember that in the logistic regression model the response variable is log odds. So we can write the equation we are trying to fit as: 

$ln(odds) = ln(\frac{p}{(1-p)}) = \beta_0 + \beta_1BMI$.

As with linear regression, the regression coefficients give the change in the predicted value for a one unit increase in the predictor variable. Except, as our predict value is a log odds ratio, the meaning the coefficients has changed. In logistic regression, the estimated regression coefficients represent log odds ratios per a unit increase in the predictor variable. We can covert these to odds ratios as follows:

```{r}
exp(coef(model.log))

```

We can see that the odds ratios for a 1 unit icrease of BMI is a `r signif(exp(coef(model.log))["bmi"], 3)` times more likely to develop Type 2 Diabetes.

Significance testing is conceptually the same as for linear regression, whereby each regression coefficient (i.e. log odds ratio) is tested to see if it is non-zero. It differs though it how it calculated. As we are no longer able to derive an exact solution, we have to use iterative method to find the best estimates. This means you sometimes might get warnings that your model failed to converge. This means that the algorithm was not able to settle on an appropriate solution for the best regression coefficients and the result should be treated with caution. Typically, this is due to not enough data or trying to fit too many predictor variables simultaneously or a poor choice of model between X and Y. 


```{r}
summary(model.log)$coefficients
```


To extract the confidence interval of the estimated regression coefficient we can use the inbuilt function `confint()`. The default is to provide the 95% confidence intervals, but we can tailor this function calculate whichever percentile we want by setting the argument `level`. If we report our estimated coefficient as an odds ratio, we also need to convert the confidence interval in exactly the same way. We can use the confidence interval to determine whether our estimated regression coefficient has a non-zero effect, by whether it contains the null value. For example at a significance level of $\alpha = 0.05$, if the estimated coefficient is significantly non zero (i.e. $p-value < 0.05$) then the 100(1-$\alpha$) = 95% confidence interval will not contain 0. The null value for the log(OR) is 0, and the null value for the OR is 1. Therefore, if we don't convert our confidence interval to the correct units we may draw the wrong conclusion. See the example below

```{r}
## First on log odds ratio scale
confint(model.log, level = 0.95)

## Second on odds ratio scale
exp(cbind(OR = coef(model.log), confint(model.log, level = 0.95)))

```

Logistic regression is all about appropriately handling the non-continuos outcome variable. The predictor variables can be as complex as your dataset can handle and include categorical variables etc as described for linear regression. 

```{r logisticRegression1, exercise=TRUE}
# Fit a logistic regression model to test for an association between age and type 2 diabetes status



```

```{r logisticRegression1-solution, exercise=TRUE}
summary(glm(t2diabetes ~ age, data = demoDat))

```

<add quiz>


```{r logisticRegression2, exercise=TRUE}
# Fit a logistic regression model to test for an association between age,alcohol units or exercise time and type 2 diabetes status


```

```{r logisticRegression2-solution, exercise=TRUE}
# Fit a logistic regression model to test for an association between age,alcohol units or exercise time and type 2 diabetes status

summary(glm(t2diabetes ~ age + exercise_hours + alcohol_units, data = demoDat))

```



```{r logisticRegression3, exercise=TRUE}
# Fit a logistic regression model to test for an association between socioeconomic status and type 2 diabetes status, controlling for age and bmi. 

model.log.full<-glm(t2diabetes ~ age + bmi + ethnicity, data = demoDat)
model.log.null<-glm(t2diabetes ~ age + bmi, data = demoDat)
anova(model.log.null, model.log.full)


```

```{r logisticRegression3-solution, exercise=TRUE}

model.log.full<-glm(t2diabetes ~ age + bmi + ethnicity, data = demoDat)
model.log.null<-glm(t2diabetes ~ age + bmi, data = demoDat)
anova(model.log.null, model.log.full)


```


#### Predictions with the logistic regression model

R has an inbuilt function to make it easy to generate predictions using the fitted regression model and new (or even old data). We are going to use this function to explore what exactly a logistic regression model is predicting and how it goes from our weighted sum of prediction variables to the binary outcome variable. 

Let's revist the example of prediction type 2 diabetes as a function of alcohol units and exercise hours. First we need to fit the model.

```{r}
model.log <- glm(t2diabetes ~ exercise_hours + alcohol_units, data = demoDat)

summary(model.log)$coefficients
```

Using our estimated regression coefficients we can write our fitted regression model as

```{r, echo = FALSE}
logEq<- paste0("$", signif(coef(model.log)[1],2), " + ", signif(coef(model.log)[2],2), " * ExerciseHours + ", 
        signif(coef(model.log)[3],2), " * AlcoholUnits$")

```

`r logEq`.

Let's say we have a new observation we want to make a prediction for, we know that they exercise for on average 4 hours a week and consume 10 units of alcohol per week. We can input these values into our equation to estimate the log odds of the this individual having type 2 diabetes. 

```{r}
## calculate log odds for individual
(logOdds <- coef(model.log)[1] + 
              coef(model.log)[2] * 4 + 
              coef(model.log)[3] * 10)

```

While by chance it's value looks like a probability we need to do some more transformations to get it to a probability. First we convert from log odds to odds:

```{r}
## convert to odds
(odds<-exp(logOdds))

```

Next, we convert from odds to probability of being a case.

```{r}
## convert to a probability
(prob<-odds/(odds+1))
```

The final step to convert this to a binary case/control status is to apply the threshold 0.5: if the probability is > 0.5 then they are classed as a case and if the probability is < 0.5 they are classed as a control. In this example the probability is just above 0.5 and therefore they would be predicted as a case. However, the estimated probability is not exactly 0, which gives a sense of how imprecise/insensitive our prediction based on this model might be. 

#### Logistic regression assumptions


The assumptions for logistic regression are:

* Dependent (i.e. outcome) variable is binary
* No outliers in continuous predictors
* No multicollinearity between predictors


Unlike with linear regression, R doesn't automatically generate plots to assess the validity of the model assumptions. The only real assumption we can check is that there is a linear relationship between continuous predictor variables and the logit of the outcome. This can be done by visually inspecting the scatter plot between each predictor and the logit values.


## Quiz

```{r finalQuiz, echo=FALSE}
quiz(caption = "Have ago at these questions to test your knowledge.",
  question("What is the interpretation of the intercept?",
    answer("8"),
    answer("14"),
    answer("1", correct = TRUE),
    answer("23")
  ),
  question("Where are you right now? (select ALL that apply)",
    answer("Planet Earth", correct = TRUE),
    answer("Pluto"),
    answer("At a computing device", correct = TRUE),
    answer("In the Milky Way", correct = TRUE),
    incorrect = "Incorrect. You're on Earth, in the Milky Way, at a computer."
  )
)
```


## Summary

In this session we have covered a number of concepts:

* What is regression
* What the intercept is


We have cover a number of different types of regression analysis

* Simple Linear Regression
* Muliple Linear Regression
* Logistic Regression



## Extras

#### Extracting summary statistics from a model fit in R

If you are new to R, here we will just run through some details on the type of objects these data are stored in and how to access specific elements. This can be helpful for writing automated analysis scripts. Due to the need to contain different types of data in different formats and structures, the output of the regression model fit is stored in a bespoke object, with slots for the the different parts of the output. These slots are named and can be assessed using the `$`. For example to extract just the table of estimated regression coefficients, which are named `coefficients` we use the following command: 


```{r}

summary(model)$coefficients
```

We can determine the type of object the coefficients table is stored in, using the function `class()`. 


```{r}
class(summary(model)$coefficients)
mode(summary(model)$coefficients)

```


The output of the command tells us it is stored in a matrix, which is a data-type in R, where you have rows and columns. A similar data-type is called a data.frame. The difference between these two data-types is that matrices can only contain one data type, which we can determine with the function `mode()`. Here it contains exclusively numeric values. In constrast, in a data frame each column can be a different data type. Our BDR data is stored in a data.frame and the output of the `str()` function, tells us the data type assigned to each column. 

Let's say we wanted to extract a single value from this matrix, there are a number of commands we can use. For example, let's extract the p-value for the age regression slope parameter using the slicing function `[`.

We can provide the row (2nd) and column (4th) number of the matrix entry we want: 
```{r}

summary(model)$coefficients[2,4]

```

Alternatively we can specify either the column or row name:

```{r, eval = FALSE}

summary(model)$coefficients["Age",4]

```


We can see a list of all components we can extract from the output of `lm()` by running `names()` on the lm object. All of these can be extracted with the `$`.

```{r}
names(model)
model$call ## example of how to extract any of the components listed by the previous command.
```

Similarly we can run `names()` on the `summary(lm)` object as showed here to get a list of all the slots available from that object. 

```{r}
names(summary(model))
```

Note these are different to those available for the model object, for example the $R^{2}$ and $\overline{R}^{2}$ are only extractable from the `summary(model)` object. 


```{r}
summary(model)$r.squared
summary(model)$adj.r.squared
```



Note that as well as directly assessing these slots using the `$` command, there also exist some predefined functions to extract the commonly requested outputs from the model fit. We have already taken advantage of one of these, `summary()`, others include `coef()`, `effects()`, `residuals()`, `fitted()` and `predict.lm()`.

### Summary model fits


Therefore, we may also want to check some of the other model summary statistics. 

In the `summary(model)` output we can see at the bottom that the results of testing the full model with an F-test. If we want to see the full table of sums of squares statistics we can use the `anova()` function on our fitted regression model.

```{r}
model<-lm(weight ~ height)
summary(model)
anova(model)

```


Comparing this table with the coefficients table, we can see that the p-value from the t-test of the age regression parameter and the F-test for the full model are identical. This is not a coincidence and is always true for the specific case of simple linear regerssion.

Finally, we will look at the $R^{2}$ and $\overline{R}^{2}$ statistics. We can see from the `summary(model)` output above these are automatically calculated. For the simple linear regression model we have fitted

$R^{2}$ = `r summary(model)$r.squared`

$\overline{R}^{2}$ = `r summary(model)$adj.r.squared`



